# llm-patch

A robust, context-aware patching engine for applying unified diffs generated by Large Language Models (LLMs). Unlike traditional patch tools, llm-patch **ignores line numbers entirely** and instead uses fuzzy, context-driven matching to apply approximate or imperfect diffs reliably.

llm-patch focuses exclusively on one problem:  
**making LLM-generated patches apply correctly in real codebases.**

---

## Why This Library Exists

LLMs routinely generate unified diffs to describe code changes. While they are remarkably good at producing correct *content*, they are notoriously unreliable at producing:

- Correct **line numbers**
- Accurately matching **context blocks**
- Perfect alignment with **current file versions**
- Stable diffs across retries or refinement loops

Traditional tools (`patch`, `git apply`) assume diffs come from `diff` itself.  
They expect exact structure and precise numbering.  
LLM diffs rarely satisfy these constraints.

The result is a breakdown in agent pipelines:  
**valid fixes cannot be applied because the diff tool fails, not because the model is wrong.**

llm-patch solves this problem by applying diffs **the way an engineer would**—by analysing the textual changes and finding the best possible location for each hunk using robust contextual and fuzzy matching, not absolute coordinates.

---

## The Landscape of Approaches

A number of commercial and open-source tools have converged on similar ideas:

- **Aider** implements a fuzzy diff application mechanism to handle imperfect patches.
- **Cursor** integrates semantic code context when aligning LLM-produced edits.
- **GitHub Copilot Agents** internally use structural and AST-aware heuristics to resolve mismatched diffs.
- **Google's Diff Match Patch** provides approximate text matching, though it does not natively consume unified diffs.
- **AST-based frameworks** (Tree-sitter, Comby, etc.) provide structural transformations but are not directly aligned with the format LLMs naturally emit.

Across the industry, the problem is well-known, but **no standalone library** has been dedicated purely to:

- Taking LLM-generated unified diffs as input  
- Applying them **without relying on line numbers**  
- Using **repeatable, measurable fuzzy logic** to ensure robustness  
- Evaluating patching strategies against a **consistent, well-defined corpus** of test cases

llm-patch exists solely to fill that gap.

This is not a general-purpose diff tool.  
This is a **specialised engine designed only for LLM-driven patching workflows**.

---

## Approach

llm-patch implements a multi-stage hunk alignment algorithm:

1. **Parse unified diffs** into individual hunks.
2. **Ignore line numbers entirely**—they are treated only as weak hints.
3. For each hunk:
   - Perform **exact matching** of the removed lines.
   - If no match is found, perform **fuzzy matching** using similarity scoring.
   - Use contextual heuristics (before/after blocks) to identify the best location.
4. Apply additions and deletions at the highest-confidence match region.
5. Emit a patched file or return structured diagnostics when ambiguity exists.

This makes the library resilient against:

- Incorrect line numbers  
- Missing or loosely matching context  
- Whitespace or formatting drift  
- Edits to the file between attempts  
- Inconsistent or partial LLM retries

---

## Grammar-Agnostic by Design

llm-patch is intentionally **grammar agnostic**. It operates purely at the textual level and can be used with:

- any programming language,  
- markup formats,  
- configuration files,  
- or arbitrary plain text.

This ensures broad applicability across agent systems.

However, llm-patch recognises that **knowledge of a language’s grammar or structure can significantly improve resilience**.  
A future extension may support **pluggable grammar modules** or integration with syntactic/semantic analyzers (e.g., Tree-sitter, AST frameworks) to provide hybrid text+structure matching strategies.

The core engine remains lightweight and language-independent, while leaving room for targeted enhancement where domain knowledge helps.

---

## Systematic Testing Methodology

A core design goal of llm-patch is **repeatability** and **quantifiable robustness**.

The project includes a growing, **well-defined corpus of test cases** drawn from real-world LLM outputs. These test cases:

- originate from actual model responses (GPT, Claude, local models),  
- cover many failure modes observed in real agentic workflows,  
- are stored in a stable dataset for deterministic benchmarking.

llm-patch’s algorithms are evaluated by running against this corpus and measuring:

- whether the diff applies without error,  
- whether the resulting file matches the expected ground truth,  
- levels of fuzz required,  
- ambiguity resolution outcomes,  
- classification of failure types.

This produces a **quantitative, repeatable measure** of patching resilience.

Rather than relying on ad-hoc, anecdotal testing, llm-patch grounds all improvement in **systematic evaluation against a curated, reproducible suite**.

---

## A Research Test Bed for Continuous Improvement

The test bed is designed as a **living dataset** of real-world LLM behaviours.  
Its purpose is to:

- capture the evolving ways models generate diffs,  
- prevent regressions across algorithm revisions,  
- provide a consistent benchmark for comparing approaches,  
- and support empirical, data-driven development.

This avoids the pitfalls of:

- tuning against random samples,  
- overfitting to isolated examples,  
- or making unverifiable claims of robustness.

Every modification to llm-patch’s algorithm must demonstrate reproducible improvement across the entire corpus.

---

## Status

This project is in early development, with the core matching engine and benchmarking harness under active construction.  
Contributions, suggestions, and additional real-world test cases are welcome.

---

## Installation

```bash
pip install llm-patch
```

For development:

```bash
git clone https://github.com/trickl/llm-patch.git
cd llm-patch
pip install -e .
pip install -r requirements-dev.txt
```

## Usage

### Basic Usage

```python
from llm_patch import apply_patch

source_code = """
def hello():
    print("Hello, World!")
"""

patch = """
def hello():
    print("Hello, Universe!")
"""

result, success = apply_patch(source_code, patch)
if success:
    print("Patch applied successfully!")
    print(result)
else:
    print("Failed to apply patch")
```

### Using PatchApplier Class

```python
from llm_patch import PatchApplier

applier = PatchApplier(similarity_threshold=0.8)
result, success = applier.apply(source_code, patch)
```

### Using FuzzyMatcher

```python
from llm_patch import FuzzyMatcher

matcher = FuzzyMatcher(threshold=0.7)
source_lines = ["line1", "line2", "line3"]
pattern_lines = ["line1", "line2"]

# Find where the pattern best matches in the source
match_index = matcher.find_best_match(source_lines, pattern_lines)
if match_index is not None:
    print(f"Pattern found at line {match_index}")
```

## Development

### Running Tests

```bash
pytest tests/ -v --cov=llm_patch
```

### Running Linters

```bash
# Run pylint
pylint src/llm_patch

# Format code with black
black src/ tests/

# Type checking with mypy
mypy src/llm_patch
```

### Pre-commit Hooks

This project uses pre-commit hooks to ensure code quality:

```bash
pre-commit install
pre-commit run --all-files
```

## Contributing

Contributions are welcome! Please feel free to submit a Pull Request.

1. Fork the repository
2. Create your feature branch (`git checkout -b feature/amazing-feature`)
3. Run tests and linters (`pytest && pylint src/llm_patch`)
4. Commit your changes (`git commit -m 'Add some amazing feature'`)
5. Push to the branch (`git push origin feature/amazing-feature`)
6. Open a Pull Request

## License

This project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.

## Acknowledgments

- Inspired by the need to reliably apply LLM-generated code patches
- Built with modern Python best practices
