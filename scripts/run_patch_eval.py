#!/usr/bin/env python3
"""Run patching algorithms (steps 4-7) on stored failure cases."""
from __future__ import annotations

import argparse
import json
import logging
import subprocess
import sys
import tempfile
from dataclasses import dataclass, field
from pathlib import Path
from typing import Dict, Iterable, Iterator, List, Sequence, Tuple

from diff_match_patch import diff_match_patch

from scripts.generate_failures import LANGUAGE_CONFIGS, LanguageConfig
from llm_patch.aider_patch import apply_aider_patch

LOGGER = logging.getLogger(__name__)
DMP = diff_match_patch()
ERROR_TOKENS = ("error", "exception", "traceback", "fatal", "syntaxerror")
ALGORITHM_LABELS = {
    "git": "Git patch",
    "diff-match-patch": "diff-match-patch",
    "aider": "Aider",
}
METRIC_SPECS = [
    ("applied", "Applied %"),
    ("fixed", "Fixed %"),
    ("compound", "Compound %"),
]
COLOR_RED = (248, 215, 218)  # bootstrap danger-100
COLOR_GREEN = (209, 231, 221)  # bootstrap success-100
NEUTRAL_COLOR = "#f8f9fa"


def get_git_hash() -> Tuple[str, str]:
    proc = subprocess.run(
        ["git", "rev-parse", "HEAD"],
        capture_output=True,
        text=True,
        check=False,
    )
    full_hash = proc.stdout.strip()
    if not full_hash:
        return ("unknown", "unknown")
    short_hash = full_hash[:7]
    return short_hash, full_hash


def pct(numerator: int, denominator: int) -> float:
    if denominator == 0:
        return 0.0
    return (numerator / denominator) * 100.0


def ratio_text_and_percent(numerator: int, denominator: int) -> tuple[str, float | None]:
    if denominator == 0:
        return "— (0/0)", None
    percentage = pct(numerator, denominator)
    return f"{percentage:5.1f}% ({numerator}/{denominator})", percentage


def metric_counts(stats: AlgorithmAggregate, metric: str) -> tuple[int, int]:
    if metric == "applied":
        return stats.applied, stats.attempts
    if metric == "fixed":
        return stats.success, stats.applied
    if metric == "compound":
        return stats.success, stats.attempts
    raise ValueError(f"Unknown metric {metric}")


def interpolate_channel(start: int, end: int, factor: float) -> int:
    return int(round(start + (end - start) * factor))


def percentage_to_color(percentage: float | None) -> str:
    if percentage is None:
        return NEUTRAL_COLOR
    clamped = max(0.0, min(percentage, 100.0)) / 100.0
    r = interpolate_channel(COLOR_RED[0], COLOR_GREEN[0], clamped)
    g = interpolate_channel(COLOR_RED[1], COLOR_GREEN[1], clamped)
    b = interpolate_channel(COLOR_RED[2], COLOR_GREEN[2], clamped)
    return f"#{r:02x}{g:02x}{b:02x}"


def render_metric_cell(stats: AlgorithmAggregate, metric: str) -> str:
    numerator, denominator = metric_counts(stats, metric)
    text, percentage = ratio_text_and_percent(numerator, denominator)
    bg_color = percentage_to_color(percentage)
    subdued = percentage is None
    color_style = "color:#6c757d;" if subdued else ""
    style = f'style="background-color:{bg_color}; text-align:center; white-space:nowrap; font-family:var(--font-mono,monospace); {color_style}"'
    return f"<td {style}>{text}</td>"


def render_markdown_table(summary: EvaluationSummary, algorithms: List[str]) -> str:
    if not summary.suites:
        return "No data to summarize."

    notice = "<!-- Auto-generated by scripts/run_patch_eval.py. Run with --markdown-report to refresh. -->"
    short_hash, full_hash = get_git_hash()
    preface = f"Results generated from commit `{short_hash}` ({full_hash})."

    lines: List[str] = [notice, "", preface, "", "<table>"]
    lines.append("  <thead>")
    lines.append("    <tr>")
    lines.append('      <th rowspan="2">Test Suite</th>')
    metric_count = len(METRIC_SPECS)
    for algo in algorithms:
        algo_label = ALGORITHM_LABELS.get(algo, algo.title())
        lines.append(f'      <th colspan="{metric_count}">{algo_label}</th>')
    lines.append("    </tr>")
    lines.append("    <tr>")
    for _ in algorithms:
        for _, metric_label in METRIC_SPECS:
            lines.append(f"      <th>{metric_label}</th>")
    lines.append("    </tr>")
    lines.append("  </thead>")
    lines.append("  <tbody>")

    sorted_keys = sorted(summary.suites.keys(), key=lambda item: (item[1], item[0]))
    for language, problem_id in sorted_keys:
        suite = summary.suites[(language, problem_id)]
        lines.append("    <tr>")
        lines.append(f"      <td><code>{problem_id}</code> · {language}</td>")
        for algo in algorithms:
            algo_stats = suite.algorithms.get(algo, AlgorithmAggregate())
            for metric_key, _ in METRIC_SPECS:
                cell_html = render_metric_cell(algo_stats, metric_key)
                lines.append(f"      {cell_html}")
        lines.append("    </tr>")

    lines.append("    <tr>")
    lines.append("      <td><strong>Average</strong></td>")
    for algo in algorithms:
        algo_stats = summary.overall.get(algo, AlgorithmAggregate())
        for metric_key, _ in METRIC_SPECS:
            cell_html = render_metric_cell(algo_stats, metric_key)
            lines.append(f"      {cell_html}")
    lines.append("    </tr>")
    lines.append("  </tbody>")
    lines.append("</table>")

    return "\n".join(lines)


@dataclass
class AlgorithmAggregate:
    attempts: int = 0
    applied: int = 0
    success: int = 0

    def record(self, patch_applied: bool, success: bool) -> None:
        self.attempts += 1
        if patch_applied:
            self.applied += 1
        if success:
            self.success += 1


@dataclass
class SuiteAggregate:
    language: str
    problem_id: str
    algorithms: Dict[str, AlgorithmAggregate] = field(default_factory=dict)


@dataclass
class EvaluationSummary:
    overall: Dict[str, AlgorithmAggregate] = field(default_factory=dict)
    suites: Dict[Tuple[str, str], SuiteAggregate] = field(default_factory=dict)

    def record(
        self,
        language: str,
        problem_id: str,
        algorithm: str,
        patch_applied: bool,
        success: bool,
    ) -> None:
        algo_stats = self.overall.setdefault(algorithm, AlgorithmAggregate())
        algo_stats.record(patch_applied, success)

        key = (language, problem_id)
        suite = self.suites.setdefault(key, SuiteAggregate(language=language, problem_id=problem_id))
        suite_stats = suite.algorithms.setdefault(algorithm, AlgorithmAggregate())
        suite_stats.record(patch_applied, success)


@dataclass(frozen=True)
class DiffStats:
    added_lines: int
    removed_lines: int
    hunks: int

    @property
    def delete_only(self) -> bool:
        return self.added_lines == 0 and self.removed_lines > 0


@dataclass
class CompileOutcome:
    returncode: int
    stdout: str
    stderr: str


@dataclass
class AttemptResult:
    case_id: str
    language: str
    model_slug: str
    algorithm: str
    diff_path: str
    patch_applied: bool
    patch_diagnostics: str | None
    compile_returncode: int | None
    errors_before: int
    errors_after: int | None
    first_error_removed: bool
    added_lines: int
    removed_lines: int
    hunks: int
    delete_only: bool
    success: bool
    notes: str


def parse_args() -> argparse.Namespace:
    parser = argparse.ArgumentParser(description="Evaluate patching algorithms on stored diffs")
    parser.add_argument(
        "--run-ids",
        help="Comma-separated run IDs under benchmarks/generated. Defaults to all runs.",
    )
    parser.add_argument(
        "--languages",
        default=",".join(sorted(LANGUAGE_CONFIGS)),
        help="Comma-separated languages to include",
    )
    parser.add_argument(
        "--models",
        help="Optional comma-separated model file slugs (e.g., qwen2.5-coder_7b) to include",
    )
    parser.add_argument(
        "--algorithms",
        default="git,diff-match-patch,aider",
        help="Comma-separated list of algorithms: git, diff-match-patch, aider",
    )
    parser.add_argument(
        "--base-dir",
        type=Path,
        default=Path("benchmarks/generated"),
        help="Base directory containing generation runs",
    )
    parser.add_argument(
        "--overwrite",
        action="store_true",
        help="Overwrite existing results files",
    )
    parser.add_argument(
        "--markdown-report",
        type=Path,
        help="Optional path to write the markdown summary table",
    )
    parser.add_argument(
        "--limit-per-language",
        type=int,
        default=0,
        help="Optional cap on number of cases per language (0 = no limit)",
    )
    parser.add_argument(
        "--log-level",
        default="INFO",
        help="Logging level (DEBUG, INFO, WARNING, ERROR)",
    )
    return parser.parse_args()


def parse_csv(value: str | None) -> List[str] | None:
    if not value:
        return None
    return [item.strip() for item in value.split(",") if item.strip()]


def canonicalize_algorithm(name: str) -> str:
    lowered = name.strip().lower()
    if lowered in {"diff-match-patch", "dmp"}:
        return "diff-match-patch"
    if lowered in {"git", "git apply"}:
        return "git"
    if lowered in {"aider", "aider-search"}:
        return "aider"
    return lowered


def discover_run_dirs(base_dir: Path, run_ids: List[str] | None) -> List[Path]:
    if run_ids:
        return [base_dir / run_id for run_id in run_ids]
    return [path for path in sorted(base_dir.iterdir()) if path.is_dir()]


def load_text(path: Path) -> str:
    if not path.exists():
        return ""
    return path.read_text(encoding="utf-8")


def extract_first_error(stderr_text: str, stdout_text: str) -> str:
    for blob in (stderr_text, stdout_text):
        for line in blob.splitlines():
            stripped = line.strip()
            if stripped:
                return stripped
    return ""


def count_error_lines(stderr_text: str, stdout_text: str) -> int:
    lines = [line.strip() for line in (stderr_text + "\n" + stdout_text).splitlines()]
    count = 0
    for line in lines:
        if not line:
            continue
        lowered = line.lower()
        if any(token in lowered for token in ERROR_TOKENS):
            count += 1
    if count == 0:
        count = sum(1 for line in lines if line)
    return count


def compute_diff_stats(diff_text: str) -> DiffStats:
    added = 0
    removed = 0
    hunks = 0
    for line in diff_text.splitlines():
        if line.startswith("+++") or line.startswith("---"):
            continue
        if line.startswith("@@"):
            hunks += 1
            continue
        if line.startswith("+"):
            added += 1
        elif line.startswith("-"):
            removed += 1
    return DiffStats(added_lines=added, removed_lines=removed, hunks=hunks)


def normalize_diff_for_git(diff_text: str, filename: str) -> str:
    lines = diff_text.splitlines()
    has_header = any(line.startswith("--- ") for line in lines)
    output: List[str] = []
    for line in lines:
        if line.startswith("--- "):
            output.append(f"--- a/{filename}")
        elif line.startswith("+++ "):
            output.append(f"+++ b/{filename}")
        else:
            output.append(line)
    if not has_header:
        output.insert(0, f"+++ b/{filename}")
        output.insert(0, f"--- a/{filename}")
    normalized = "\n".join(output)
    if not normalized.endswith("\n"):
        normalized += "\n"
    return normalized


def apply_git_patch(source_text: str, diff_text: str, filename: str) -> tuple[str, bool, str]:
    normalized_diff = normalize_diff_for_git(diff_text, filename)
    with tempfile.TemporaryDirectory(prefix="llm_patch_git_") as tmpdir:
        tmp_path = Path(tmpdir)
        subprocess.run(["git", "init", "-q"], cwd=tmp_path, check=False)
        file_path = tmp_path / filename
        file_path.write_text(source_text, encoding="utf-8")
        patch_path = tmp_path / "patch.diff"
        patch_path.write_text(normalized_diff, encoding="utf-8")
        proc = subprocess.run(
            ["git", "apply", "--allow-empty", "--whitespace=nowarn", patch_path.name],
            cwd=tmp_path,
            capture_output=True,
            text=True,
            check=False,
        )
        if proc.returncode != 0:
            diagnostics = proc.stderr.strip() or proc.stdout.strip() or "git apply failed"
            return source_text, False, diagnostics
        result_text = file_path.read_text(encoding="utf-8")
        diagnostics = proc.stdout.strip() or "applied"
        return result_text, True, diagnostics


def convert_unified_to_dmp(diff_text: str) -> str:
    useful_lines: List[str] = []
    saw_hunk = False
    for line in diff_text.splitlines():
        if line.startswith("@@"):
            saw_hunk = True
            useful_lines.append(line)
            continue
        if not saw_hunk:
            continue
        if line.startswith("+++") or line.startswith("---"):
            continue
        if line.startswith((" ", "+", "-")):
            useful_lines.append(line)
    if not useful_lines:
        raise ValueError("Diff does not contain any hunks")
    patch_text = "\n".join(useful_lines)
    if not patch_text.endswith("\n"):
        patch_text += "\n"
    return patch_text


def apply_dmp_patch(source_text: str, diff_text: str) -> tuple[str, bool, str]:
    try:
        dmp_patch_text = convert_unified_to_dmp(diff_text)
    except ValueError as exc:  # noqa: PERF203 - informative exception
        return source_text, False, str(exc)
    try:
        patches = DMP.patch_fromText(dmp_patch_text)
    except ValueError as exc:  # noqa: PERF203
        return source_text, False, f"DMP parse error: {exc}"
    patched_text, statuses = DMP.patch_apply(patches, source_text)
    success = all(statuses)
    diagnostics = f"statuses={statuses}"
    return patched_text, success, diagnostics


def run_compile(code: str, language_cfg: LanguageConfig, compile_cmd: Sequence[str]) -> CompileOutcome:
    with tempfile.TemporaryDirectory(prefix="llm_patch_compile_") as tmpdir:
        tmp_path = Path(tmpdir)
        source_path = tmp_path / language_cfg.filename
        source_path.write_text(code, encoding="utf-8")
        proc = subprocess.run(compile_cmd, cwd=tmp_path, capture_output=True, text=True, check=False)
    return CompileOutcome(returncode=proc.returncode, stdout=proc.stdout, stderr=proc.stderr)


def ensure_results_dir(case_dir: Path) -> Path:
    results_dir = case_dir / "results"
    results_dir.mkdir(exist_ok=True)
    return results_dir


def should_mark_success(
    patch_applied: bool,
    stats: DiffStats,
    first_error_removed: bool,
    errors_before: int,
    errors_after: int | None,
) -> bool:
    if not patch_applied:
        return False
    if stats.delete_only:
        return False
    if not first_error_removed:
        return False
    if errors_after is None:
        return False
    return errors_after < errors_before


def evaluate_case(
    case_dir: Path,
    language_cfg: LanguageConfig,
    algorithms: List[str],
    model_filter: List[str] | None,
    overwrite: bool,
    summary: EvaluationSummary,
) -> None:
    before_path = case_dir / f"before.{language_cfg.extension}"
    if not before_path.exists():
        LOGGER.debug("Skipping %s (missing before file)", case_dir)
        return
    before_code = before_path.read_text(encoding="utf-8")
    stderr_text = load_text(case_dir / "compiler_stderr.txt")
    stdout_text = load_text(case_dir / "compiler_stdout.txt")
    manifest_path = case_dir / "manifest.json"
    if not manifest_path.exists():
        LOGGER.warning("Missing manifest for %s", case_dir)
        return
    manifest = json.loads(manifest_path.read_text(encoding="utf-8"))
    compile_cmd = manifest.get("compile_command")
    if not isinstance(compile_cmd, list) or not compile_cmd:
        LOGGER.warning("Malformed compile command for %s", case_dir)
        return
    problem_id = manifest.get("problem_id", "unknown")
    first_error = extract_first_error(stderr_text, stdout_text)
    errors_before = count_error_lines(stderr_text, stdout_text)

    diffs_dir = case_dir / "diffs"
    if not diffs_dir.exists():
        LOGGER.debug("No diffs for %s", case_dir)
        return

    for diff_path in sorted(diffs_dir.glob("*.diff")):
        model_slug = diff_path.stem
        if model_filter and model_slug not in model_filter:
            continue
        diff_text = diff_path.read_text(encoding="utf-8")
        stats = compute_diff_stats(diff_text)
        results_dir = ensure_results_dir(case_dir)
        for algorithm in algorithms:
            algo_key = algorithm.lower()
            result_path = results_dir / f"{model_slug}__{algo_key}.json"
            if result_path.exists() and not overwrite:
                LOGGER.debug("Using cached result %s", result_path)
                cached = json.loads(result_path.read_text(encoding="utf-8"))
                summary.record(
                    language=manifest["language"],
                    problem_id=problem_id,
                    algorithm=algo_key,
                    patch_applied=bool(cached.get("patch_applied", False)),
                    success=bool(cached.get("success", False)),
                )
                continue
            patched_code: str
            patch_applied: bool
            diagnostics: str
            if algo_key == "git":
                patched_code, patch_applied, diagnostics = apply_git_patch(before_code, diff_text, language_cfg.filename)
            elif algo_key in {"diff-match-patch", "dmp"}:
                patched_code, patch_applied, diagnostics = apply_dmp_patch(before_code, diff_text)
            elif algo_key == "aider":
                patched_code, patch_applied, diagnostics = apply_aider_patch(before_code, diff_text)
            else:
                LOGGER.error("Unknown algorithm %s", algorithm)
                continue
            compile_outcome: CompileOutcome | None = None
            errors_after: int | None = None
            first_error_removed = False
            if patch_applied:
                compile_outcome = run_compile(patched_code, language_cfg, compile_cmd)
                errors_after = count_error_lines(compile_outcome.stderr, compile_outcome.stdout)
                if first_error:
                    combined = compile_outcome.stderr + compile_outcome.stdout
                    first_error_removed = first_error not in combined
                else:
                    first_error_removed = True
            notes = ""
            if stats.delete_only:
                notes = "diff only deletes lines"
            result = AttemptResult(
                case_id=manifest["case_id"],
                language=manifest["language"],
                model_slug=model_slug,
                algorithm=algo_key,
                diff_path=str(diff_path.relative_to(case_dir)),
                patch_applied=patch_applied,
                patch_diagnostics=diagnostics,
                compile_returncode=None if compile_outcome is None else compile_outcome.returncode,
                errors_before=errors_before,
                errors_after=errors_after,
                first_error_removed=first_error_removed,
                added_lines=stats.added_lines,
                removed_lines=stats.removed_lines,
                hunks=stats.hunks,
                delete_only=stats.delete_only,
                success=should_mark_success(patch_applied, stats, first_error_removed, errors_before, errors_after),
                notes=notes,
            )
            summary.record(
                language=manifest["language"],
                problem_id=problem_id,
                algorithm=algo_key,
                patch_applied=patch_applied,
                success=result.success,
            )
            result_path.write_text(json.dumps(result.__dict__, indent=2), encoding="utf-8")
            LOGGER.debug(
                "%s | %s | %s -> success=%s",
                manifest["case_id"],
                model_slug,
                algo_key,
                result.success,
            )


def iterate_case_dirs(run_dirs: Iterable[Path], languages: List[str], limit_per_language: int) -> Iterator[tuple[Path, LanguageConfig]]:
    per_language_counts: Dict[str, int] = {lang: 0 for lang in languages}
    for run_dir in run_dirs:
        for language in languages:
            lang_dir = run_dir / language
            if not lang_dir.exists():
                continue
            for case_dir in sorted(lang_dir.iterdir()):
                if not case_dir.is_dir():
                    continue
                if limit_per_language and per_language_counts[language] >= limit_per_language:
                    break
                per_language_counts[language] += 1
                yield case_dir, LANGUAGE_CONFIGS[language]


def main() -> None:
    args = parse_args()
    logging.basicConfig(level=getattr(logging, args.log_level.upper(), logging.INFO), format="[%(levelname)s] %(message)s")
    languages = parse_csv(args.languages) or []
    for language in languages:
        if language not in LANGUAGE_CONFIGS:
            raise ValueError(f"Unsupported language '{language}'")
    algorithms_raw = parse_csv(args.algorithms) or []
    algorithms = []
    for algo in algorithms_raw:
        canonical = canonicalize_algorithm(algo)
        if canonical not in algorithms:
            algorithms.append(canonical)
    run_ids = parse_csv(args.run_ids)
    model_filter = parse_csv(args.models)

    run_dirs = discover_run_dirs(args.base_dir, run_ids)
    summary = EvaluationSummary()

    for case_dir, language_cfg in iterate_case_dirs(run_dirs, languages, args.limit_per_language):
        evaluate_case(case_dir, language_cfg, algorithms, model_filter, args.overwrite, summary)

    if not summary.overall:
        LOGGER.info("No attempts were recorded (nothing to do).")
        return

    LOGGER.info("=== Summary ===")
    for algo in algorithms:
        stats = summary.overall.get(algo, AlgorithmAggregate())
        attempts = stats.attempts
        applied = stats.applied
        success = stats.success
        LOGGER.info(
            "%s: attempts=%s applied=%s successful=%s (%.2f%%)",
            algo,
            attempts,
            applied,
            success,
            pct(success, attempts),
        )

    table_md = render_markdown_table(summary, algorithms)
    print("\n" + table_md + "\n")

    if args.markdown_report:
        args.markdown_report.parent.mkdir(parents=True, exist_ok=True)
        args.markdown_report.write_text(table_md + "\n", encoding="utf-8")
        LOGGER.info("Markdown report written to %s", args.markdown_report)


if __name__ == "__main__":
    try:
        main()
    except KeyboardInterrupt:
        sys.exit(130)
